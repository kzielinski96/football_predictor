{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs part\n",
    "### Same predictions as in the earlier parts using recurrent neural network with tools provided by Keras library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obsługa środowisk Python 2 i Python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Importowanie popularnych modułów\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# W celu zachowania powtarzalności wyników w kolejnych przebiegach\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generowanie ładnych wykresów\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Lokacja, w której będą zapisywane rysunki\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"preparing_dataset\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"pictures\", CHAPTER_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"pictures\", CHAPTER_ID, fig_id)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving an image\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "FOOTBALL_PATH_SP = os.path.join(\"datasets\", \"spain\")\n",
    "football_path_sp = FOOTBALL_PATH_SP\n",
    "\n",
    "FOOTBALL_PATH_EN = os.path.join(\"datasets\", \"england\")\n",
    "football_path_en = FOOTBALL_PATH_EN\n",
    "\n",
    "FOOTBALL_PATH_FR = os.path.join(\"datasets\", \"france\")\n",
    "football_path_fr = FOOTBALL_PATH_FR\n",
    "\n",
    "FOOTBALL_PATH_GE = os.path.join(\"datasets\", \"germany\")\n",
    "football_path_ge = FOOTBALL_PATH_GE\n",
    "\n",
    "FOOTBALL_PATH_IT = os.path.join(\"datasets\", \"italy\")\n",
    "football_path_it = FOOTBALL_PATH_IT\n",
    "\n",
    "def load_football_data(football_path, file):\n",
    "    csv_path = os.path.join(football_path, file)\n",
    "    return pd.read_csv(csv_path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_sp = load_football_data(FOOTBALL_PATH_SP, \"spain.csv\")\n",
    "football_en = load_football_data(FOOTBALL_PATH_EN, \"england.csv\")\n",
    "football_fr = load_football_data(FOOTBALL_PATH_FR, \"france.csv\")\n",
    "football_ge = load_football_data(FOOTBALL_PATH_GE, \"germany.csv\")\n",
    "football_it = load_football_data(FOOTBALL_PATH_IT, \"italy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "football = football_it.copy()\n",
    "football = football.dropna(subset=[\"Date\"])\n",
    "football = pd.DataFrame(football).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5004 entries, 0 to 5003\n",
      "Data columns (total 26 columns):\n",
      "Div         5004 non-null object\n",
      "Date        5004 non-null object\n",
      "HomeTeam    5004 non-null object\n",
      "AwayTeam    5004 non-null object\n",
      "FTHG        5004 non-null float64\n",
      "FTAG        5004 non-null float64\n",
      "FTR         5004 non-null object\n",
      "HTHG        5004 non-null float64\n",
      "HTAG        5004 non-null float64\n",
      "HTR         5004 non-null object\n",
      "HS          5004 non-null float64\n",
      "AS          5004 non-null float64\n",
      "HST         5004 non-null float64\n",
      "AST         5004 non-null float64\n",
      "HF          5004 non-null float64\n",
      "AF          5004 non-null float64\n",
      "HY          5004 non-null float64\n",
      "AY          5004 non-null float64\n",
      "HR          5004 non-null float64\n",
      "AR          5004 non-null float64\n",
      "B365H       5004 non-null float64\n",
      "B365D       5004 non-null float64\n",
      "B365A       5004 non-null float64\n",
      "BWH         5004 non-null float64\n",
      "BWD         5004 non-null float64\n",
      "BWA         5004 non-null float64\n",
      "dtypes: float64(20), object(6)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "football.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "homeTeamList = football[\"HomeTeam\"].tolist() \n",
    "awayTeamList = football[\"AwayTeam\"].tolist()\n",
    "fTRList = football[\"FTR\"].tolist()\n",
    "hTRList = football[\"HTR\"].tolist()\n",
    "divList = football[\"Div\"].tolist()\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "\n",
    "labelEncoder.fit(homeTeamList)\n",
    "label = labelEncoder.transform(homeTeamList)\n",
    "football['homeTeam']=pd.Series(label)\n",
    "\n",
    "labelEncoder.fit(awayTeamList)\n",
    "label = labelEncoder.transform(awayTeamList)\n",
    "football['awayTeam']=pd.Series(label)\n",
    "\n",
    "labelEncoder.fit(hTRList)\n",
    "label = labelEncoder.transform(hTRList)\n",
    "football['hTR']=pd.Series(label)\n",
    "\n",
    "labelEncoder.fit(fTRList)\n",
    "label = labelEncoder.transform(fTRList)\n",
    "football['fTR']=pd.Series(label)\n",
    "\n",
    "labelEncoder.fit(divList)\n",
    "label = labelEncoder.transform(divList)\n",
    "football['div']=pd.Series(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>...</th>\n",
       "      <th>BWH</th>\n",
       "      <th>BWD</th>\n",
       "      <th>BWA</th>\n",
       "      <th>homeTeam</th>\n",
       "      <th>awayTeam</th>\n",
       "      <th>hTR</th>\n",
       "      <th>fTR</th>\n",
       "      <th>div</th>\n",
       "      <th>DayOfTheYear</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I1</td>\n",
       "      <td>27/08/05</td>\n",
       "      <td>Fiorentina</td>\n",
       "      <td>Sampdoria</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I1</td>\n",
       "      <td>27/08/05</td>\n",
       "      <td>Livorno</td>\n",
       "      <td>Lecce</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.80</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I1</td>\n",
       "      <td>28/08/05</td>\n",
       "      <td>Ascoli</td>\n",
       "      <td>Milan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I1</td>\n",
       "      <td>28/08/05</td>\n",
       "      <td>Inter</td>\n",
       "      <td>Treviso</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>1.16</td>\n",
       "      <td>5.90</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I1</td>\n",
       "      <td>28/08/05</td>\n",
       "      <td>Juventus</td>\n",
       "      <td>Chievo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I1</td>\n",
       "      <td>28/08/05</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>Messina</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.10</td>\n",
       "      <td>4.00</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I1</td>\n",
       "      <td>28/08/05</td>\n",
       "      <td>Parma</td>\n",
       "      <td>Palermo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.75</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I1</td>\n",
       "      <td>28/08/05</td>\n",
       "      <td>Reggina</td>\n",
       "      <td>Roma</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.90</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I1</td>\n",
       "      <td>28/08/05</td>\n",
       "      <td>Siena</td>\n",
       "      <td>Cagliari</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.30</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I1</td>\n",
       "      <td>28/08/05</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>Empoli</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.40</td>\n",
       "      <td>5.50</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I1</td>\n",
       "      <td>10/09/05</td>\n",
       "      <td>Milan</td>\n",
       "      <td>Siena</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>1.16</td>\n",
       "      <td>6.40</td>\n",
       "      <td>12.50</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I1</td>\n",
       "      <td>10/09/05</td>\n",
       "      <td>Palermo</td>\n",
       "      <td>Inter</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.05</td>\n",
       "      <td>2.00</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I1</td>\n",
       "      <td>11/09/05</td>\n",
       "      <td>Cagliari</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.45</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I1</td>\n",
       "      <td>11/09/05</td>\n",
       "      <td>Chievo</td>\n",
       "      <td>Parma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.45</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I1</td>\n",
       "      <td>11/09/05</td>\n",
       "      <td>Empoli</td>\n",
       "      <td>Juventus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>6.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.45</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I1</td>\n",
       "      <td>11/09/05</td>\n",
       "      <td>Lecce</td>\n",
       "      <td>Ascoli</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75</td>\n",
       "      <td>3.10</td>\n",
       "      <td>4.90</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I1</td>\n",
       "      <td>11/09/05</td>\n",
       "      <td>Messina</td>\n",
       "      <td>Fiorentina</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.70</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I1</td>\n",
       "      <td>11/09/05</td>\n",
       "      <td>Roma</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.50</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I1</td>\n",
       "      <td>11/09/05</td>\n",
       "      <td>Sampdoria</td>\n",
       "      <td>Reggina</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.60</td>\n",
       "      <td>6.50</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I1</td>\n",
       "      <td>11/09/05</td>\n",
       "      <td>Treviso</td>\n",
       "      <td>Livorno</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.90</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Div      Date    HomeTeam    AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  ...  \\\n",
       "0   I1  27/08/05  Fiorentina   Sampdoria   2.0   1.0   H   2.0   0.0   H  ...   \n",
       "1   I1  27/08/05     Livorno       Lecce   2.0   1.0   H   1.0   1.0   D  ...   \n",
       "2   I1  28/08/05      Ascoli       Milan   1.0   1.0   D   0.0   0.0   D  ...   \n",
       "3   I1  28/08/05       Inter     Treviso   3.0   0.0   H   1.0   0.0   H  ...   \n",
       "4   I1  28/08/05    Juventus      Chievo   1.0   0.0   H   1.0   0.0   H  ...   \n",
       "5   I1  28/08/05       Lazio     Messina   1.0   0.0   H   1.0   0.0   H  ...   \n",
       "6   I1  28/08/05       Parma     Palermo   1.0   1.0   D   0.0   1.0   A  ...   \n",
       "7   I1  28/08/05     Reggina        Roma   0.0   3.0   A   0.0   1.0   A  ...   \n",
       "8   I1  28/08/05       Siena    Cagliari   2.0   1.0   H   1.0   1.0   D  ...   \n",
       "9   I1  28/08/05     Udinese      Empoli   1.0   0.0   H   1.0   0.0   H  ...   \n",
       "10  I1  10/09/05       Milan       Siena   3.0   1.0   H   2.0   1.0   H  ...   \n",
       "11  I1  10/09/05     Palermo       Inter   3.0   2.0   H   1.0   0.0   H  ...   \n",
       "12  I1  11/09/05    Cagliari       Lazio   1.0   1.0   D   1.0   1.0   D  ...   \n",
       "13  I1  11/09/05      Chievo       Parma   1.0   0.0   H   1.0   0.0   H  ...   \n",
       "14  I1  11/09/05      Empoli    Juventus   0.0   4.0   A   0.0   3.0   A  ...   \n",
       "15  I1  11/09/05       Lecce      Ascoli   0.0   0.0   D   0.0   0.0   D  ...   \n",
       "16  I1  11/09/05     Messina  Fiorentina   2.0   2.0   D   0.0   2.0   A  ...   \n",
       "17  I1  11/09/05        Roma     Udinese   0.0   1.0   A   0.0   1.0   A  ...   \n",
       "18  I1  11/09/05   Sampdoria     Reggina   3.0   2.0   H   1.0   1.0   D  ...   \n",
       "19  I1  11/09/05     Treviso     Livorno   0.0   1.0   A   0.0   0.0   D  ...   \n",
       "\n",
       "     BWH   BWD    BWA  homeTeam  awayTeam  hTR  fTR  div  DayOfTheYear  Year  \n",
       "0   2.25  2.90   3.20        13        30    3    2    0           239  2005  \n",
       "1   1.90  3.20   3.80        20        19    2    2    0           239  2005  \n",
       "2   9.00  4.50   1.30         0        22    2    1    0           240  2005  \n",
       "3   1.16  5.90  15.00        16        35    3    2    0           240  2005  \n",
       "4   1.22  5.50  10.00        17        10    3    2    0           240  2005  \n",
       "5   1.90  3.10   4.00        18        21    3    2    0           240  2005  \n",
       "6   2.55  2.90   2.75        26        25    1    1    0           240  2005  \n",
       "7   3.90  3.15   1.90        28        29    1    0    0           240  2005  \n",
       "8   2.20  2.90   3.30        32         6    2    2    0           240  2005  \n",
       "9   1.60  3.40   5.50        36        12    3    2    0           240  2005  \n",
       "10  1.16  6.40  12.50        22        32    3    2    0           253  2005  \n",
       "11  3.65  3.05   2.00        25        16    3    2    0           253  2005  \n",
       "12  2.15  2.90   3.45         6        18    2    1    0           254  2005  \n",
       "13  2.25  2.75   3.45        10        26    3    2    0           254  2005  \n",
       "14  6.75  3.75   1.45        12        17    1    0    0           254  2005  \n",
       "15  1.75  3.10   4.90        19         0    2    1    0           254  2005  \n",
       "16  2.70  2.80   2.70        21        13    1    1    0           254  2005  \n",
       "17  1.77  3.20   4.50        29        36    1    0    0           254  2005  \n",
       "18  1.50  3.60   6.50        30        28    2    2    0           254  2005  \n",
       "19  2.50  2.80   2.90        35        20    2    0    0           254  2005  \n",
       "\n",
       "[20 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "dates = pd.Series(football['Date'])\n",
    "dates = pd.to_datetime(dates, format = '%d/%m/%y')\n",
    "days = []\n",
    "years = []\n",
    "\n",
    "for i in dates:\n",
    "    d = i.dayofyear\n",
    "    days.append(d)\n",
    "    y = i.year\n",
    "    years.append(y)\n",
    "    \n",
    "x = pd.Series(days)\n",
    "y = pd.Series(years)\n",
    "football[\"DayOfTheYear\"] = x\n",
    "football[\"Year\"] = y\n",
    "football.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "dates = pd.Series(football['Date'])\n",
    "dates = pd.to_datetime(dates, format = '%d/%m/%y')\n",
    "days = []\n",
    "years = []\n",
    "\n",
    "for i in dates:\n",
    "    d = i.dayofyear\n",
    "    days.append(d)\n",
    "    y = i.year\n",
    "    years.append(y)\n",
    "    \n",
    "x = pd.Series(days)\n",
    "y = pd.Series(years)\n",
    "football[\"DayOfTheYear\"] = x\n",
    "football[\"Year\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "football = football.drop(columns = ['div', 'Div','Date', 'HomeTeam', 'AwayTeam', 'HTR', 'FTR', 'FTHG', 'HTHG','FTAG', 'HTAG'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>HST</th>\n",
       "      <th>AST</th>\n",
       "      <th>HF</th>\n",
       "      <th>AF</th>\n",
       "      <th>HY</th>\n",
       "      <th>AY</th>\n",
       "      <th>HR</th>\n",
       "      <th>AR</th>\n",
       "      <th>...</th>\n",
       "      <th>B365A</th>\n",
       "      <th>BWH</th>\n",
       "      <th>BWD</th>\n",
       "      <th>BWA</th>\n",
       "      <th>homeTeam</th>\n",
       "      <th>awayTeam</th>\n",
       "      <th>hTR</th>\n",
       "      <th>fTR</th>\n",
       "      <th>DayOfTheYear</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>239</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>239</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.40</td>\n",
       "      <td>9.00</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1.16</td>\n",
       "      <td>5.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>240</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HS    AS  HST  AST    HF    AF   HY   AY   HR   AR  ...  B365A   BWH  \\\n",
       "0  15.0   9.0  9.0  4.0  18.0  23.0  1.0  4.0  0.0  0.0  ...   3.25  2.25   \n",
       "1  17.0   6.0  7.0  5.0  27.0  21.0  2.0  3.0  0.0  0.0  ...   4.00  1.90   \n",
       "2   8.0  16.0  3.0  9.0  22.0  16.0  2.0  1.0  0.0  0.0  ...   1.40  9.00   \n",
       "3  16.0   7.0  9.0  3.0  13.0  20.0  1.0  3.0  0.0  0.0  ...  13.00  1.16   \n",
       "4  16.0   2.0  7.0  0.0  16.0  12.0  1.0  2.0  0.0  0.0  ...  13.00  1.22   \n",
       "\n",
       "   BWD   BWA  homeTeam  awayTeam  hTR  fTR  DayOfTheYear  Year  \n",
       "0  2.9   3.2        13        30    3    2           239  2005  \n",
       "1  3.2   3.8        20        19    2    2           239  2005  \n",
       "2  4.5   1.3         0        22    2    1           240  2005  \n",
       "3  5.9  15.0        16        35    3    2           240  2005  \n",
       "4  5.5  10.0        17        10    3    2           240  2005  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "football.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x,y = football.loc[:,football.columns != 'fTR'], football.loc[:,'fTR']\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the input data in order to make it work with rnns (expected input is [batch_size, n_steps, n_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(float)\n",
    "x_test = x_test.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kub/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3502/3502 [==============================] - 5s 1ms/sample - loss: 1.5927 - acc: 0.3824\n",
      "Epoch 2/200\n",
      "3502/3502 [==============================] - 3s 941us/sample - loss: 1.0805 - acc: 0.4523\n",
      "Epoch 3/200\n",
      "3502/3502 [==============================] - 3s 794us/sample - loss: 1.0117 - acc: 0.4977\n",
      "Epoch 4/200\n",
      "3502/3502 [==============================] - 2s 675us/sample - loss: 1.0098 - acc: 0.5134\n",
      "Epoch 5/200\n",
      "3502/3502 [==============================] - 3s 722us/sample - loss: 0.9443 - acc: 0.5403\n",
      "Epoch 6/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.9517 - acc: 0.5414\n",
      "Epoch 7/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.9257 - acc: 0.5640\n",
      "Epoch 8/200\n",
      "3502/3502 [==============================] - 2s 675us/sample - loss: 0.9193 - acc: 0.5697\n",
      "Epoch 9/200\n",
      "3502/3502 [==============================] - 3s 715us/sample - loss: 0.8737 - acc: 0.5922\n",
      "Epoch 10/200\n",
      "3502/3502 [==============================] - 2s 673us/sample - loss: 0.9184 - acc: 0.5902\n",
      "Epoch 11/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.8758 - acc: 0.5862\n",
      "Epoch 12/200\n",
      "3502/3502 [==============================] - 2s 697us/sample - loss: 0.8555 - acc: 0.5937\n",
      "Epoch 13/200\n",
      "3502/3502 [==============================] - 2s 695us/sample - loss: 0.8428 - acc: 0.6068\n",
      "Epoch 14/200\n",
      "3502/3502 [==============================] - 3s 728us/sample - loss: 0.8215 - acc: 0.6159\n",
      "Epoch 15/200\n",
      "3502/3502 [==============================] - 3s 727us/sample - loss: 0.8232 - acc: 0.6162\n",
      "Epoch 16/200\n",
      "3502/3502 [==============================] - 2s 695us/sample - loss: 0.8195 - acc: 0.6174\n",
      "Epoch 17/200\n",
      "3502/3502 [==============================] - 2s 679us/sample - loss: 0.8131 - acc: 0.6182\n",
      "Epoch 18/200\n",
      "3502/3502 [==============================] - 2s 677us/sample - loss: 0.8169 - acc: 0.6196\n",
      "Epoch 19/200\n",
      "3502/3502 [==============================] - 2s 697us/sample - loss: 0.8042 - acc: 0.6256\n",
      "Epoch 20/200\n",
      "3502/3502 [==============================] - 2s 683us/sample - loss: 0.7855 - acc: 0.6376\n",
      "Epoch 21/200\n",
      "3502/3502 [==============================] - 2s 683us/sample - loss: 0.7817 - acc: 0.6374\n",
      "Epoch 22/200\n",
      "3502/3502 [==============================] - 2s 697us/sample - loss: 0.8077 - acc: 0.6248\n",
      "Epoch 23/200\n",
      "3502/3502 [==============================] - 2s 683us/sample - loss: 0.7711 - acc: 0.6411\n",
      "Epoch 24/200\n",
      "3502/3502 [==============================] - 2s 700us/sample - loss: 0.7829 - acc: 0.6374\n",
      "Epoch 25/200\n",
      "3502/3502 [==============================] - 2s 699us/sample - loss: 0.7694 - acc: 0.6442\n",
      "Epoch 26/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.7829 - acc: 0.6428\n",
      "Epoch 27/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.7597 - acc: 0.6513\n",
      "Epoch 28/200\n",
      "3502/3502 [==============================] - 2s 708us/sample - loss: 0.7666 - acc: 0.6391\n",
      "Epoch 29/200\n",
      "3502/3502 [==============================] - 2s 687us/sample - loss: 0.8075 - acc: 0.6288\n",
      "Epoch 30/200\n",
      "3502/3502 [==============================] - 2s 684us/sample - loss: 0.7805 - acc: 0.6354\n",
      "Epoch 31/200\n",
      "3502/3502 [==============================] - 3s 743us/sample - loss: 0.7876 - acc: 0.6402\n",
      "Epoch 32/200\n",
      "3502/3502 [==============================] - 3s 716us/sample - loss: 0.7698 - acc: 0.6359\n",
      "Epoch 33/200\n",
      "3502/3502 [==============================] - 2s 685us/sample - loss: 0.7571 - acc: 0.6542\n",
      "Epoch 34/200\n",
      "3502/3502 [==============================] - 2s 702us/sample - loss: 0.8081 - acc: 0.6259\n",
      "Epoch 35/200\n",
      "3502/3502 [==============================] - 2s 692us/sample - loss: 0.7701 - acc: 0.6374\n",
      "Epoch 36/200\n",
      "3502/3502 [==============================] - 2s 684us/sample - loss: 0.7593 - acc: 0.6516\n",
      "Epoch 37/200\n",
      "3502/3502 [==============================] - 2s 686us/sample - loss: 0.7673 - acc: 0.6491\n",
      "Epoch 38/200\n",
      "3502/3502 [==============================] - 2s 683us/sample - loss: 0.7632 - acc: 0.6539\n",
      "Epoch 39/200\n",
      "3502/3502 [==============================] - 2s 689us/sample - loss: 0.7547 - acc: 0.6485\n",
      "Epoch 40/200\n",
      "3502/3502 [==============================] - 2s 684us/sample - loss: 0.7664 - acc: 0.6462\n",
      "Epoch 41/200\n",
      "3502/3502 [==============================] - 2s 686us/sample - loss: 0.7860 - acc: 0.6428\n",
      "Epoch 42/200\n",
      "3502/3502 [==============================] - 2s 688us/sample - loss: 0.7482 - acc: 0.6551\n",
      "Epoch 43/200\n",
      "3502/3502 [==============================] - 2s 687us/sample - loss: 0.7671 - acc: 0.6433\n",
      "Epoch 44/200\n",
      "3502/3502 [==============================] - 2s 683us/sample - loss: 0.7642 - acc: 0.6471\n",
      "Epoch 45/200\n",
      "3502/3502 [==============================] - 2s 692us/sample - loss: 0.7593 - acc: 0.6436\n",
      "Epoch 46/200\n",
      "3502/3502 [==============================] - 2s 684us/sample - loss: 0.7669 - acc: 0.6459\n",
      "Epoch 47/200\n",
      "3502/3502 [==============================] - 2s 707us/sample - loss: 0.7680 - acc: 0.6473\n",
      "Epoch 48/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.7621 - acc: 0.6522\n",
      "Epoch 49/200\n",
      "3502/3502 [==============================] - 2s 685us/sample - loss: 0.7503 - acc: 0.6568\n",
      "Epoch 50/200\n",
      "3502/3502 [==============================] - 2s 687us/sample - loss: 0.7502 - acc: 0.6625\n",
      "Epoch 51/200\n",
      "3502/3502 [==============================] - 2s 688us/sample - loss: 0.7580 - acc: 0.6448\n",
      "Epoch 52/200\n",
      "3502/3502 [==============================] - 2s 686us/sample - loss: 0.7703 - acc: 0.6451\n",
      "Epoch 53/200\n",
      "3502/3502 [==============================] - 3s 734us/sample - loss: 0.7543 - acc: 0.6485\n",
      "Epoch 54/200\n",
      "3502/3502 [==============================] - 3s 723us/sample - loss: 0.7485 - acc: 0.6588\n",
      "Epoch 55/200\n",
      "3502/3502 [==============================] - 3s 737us/sample - loss: 0.7589 - acc: 0.6502\n",
      "Epoch 56/200\n",
      "3502/3502 [==============================] - 3s 840us/sample - loss: 0.7661 - acc: 0.6471\n",
      "Epoch 57/200\n",
      "3502/3502 [==============================] - 3s 737us/sample - loss: 0.7738 - acc: 0.6391\n",
      "Epoch 58/200\n",
      "3502/3502 [==============================] - 3s 734us/sample - loss: 0.7468 - acc: 0.6565\n",
      "Epoch 59/200\n",
      "3502/3502 [==============================] - 3s 728us/sample - loss: 0.7622 - acc: 0.6411\n",
      "Epoch 60/200\n",
      "3502/3502 [==============================] - 3s 735us/sample - loss: 0.7538 - acc: 0.6551\n",
      "Epoch 61/200\n",
      "3502/3502 [==============================] - 3s 737us/sample - loss: 0.7506 - acc: 0.6539\n",
      "Epoch 62/200\n",
      "3502/3502 [==============================] - 3s 792us/sample - loss: 0.7620 - acc: 0.6468\n",
      "Epoch 63/200\n",
      "3502/3502 [==============================] - 3s 787us/sample - loss: 0.7428 - acc: 0.6562\n",
      "Epoch 64/200\n",
      "3502/3502 [==============================] - 3s 789us/sample - loss: 0.7544 - acc: 0.6525\n",
      "Epoch 65/200\n",
      "3502/3502 [==============================] - 3s 787us/sample - loss: 0.7550 - acc: 0.6533\n",
      "Epoch 66/200\n",
      "3502/3502 [==============================] - 3s 749us/sample - loss: 0.7514 - acc: 0.6548\n",
      "Epoch 67/200\n",
      "3502/3502 [==============================] - 3s 784us/sample - loss: 0.7464 - acc: 0.6568\n",
      "Epoch 68/200\n",
      "3502/3502 [==============================] - 3s 787us/sample - loss: 0.7538 - acc: 0.6625\n",
      "Epoch 69/200\n",
      "3502/3502 [==============================] - 3s 838us/sample - loss: 0.7573 - acc: 0.6591\n",
      "Epoch 70/200\n",
      "3502/3502 [==============================] - 3s 736us/sample - loss: 0.7431 - acc: 0.6593\n",
      "Epoch 71/200\n",
      "3502/3502 [==============================] - 3s 746us/sample - loss: 0.7592 - acc: 0.6522\n",
      "Epoch 72/200\n",
      "3502/3502 [==============================] - 3s 826us/sample - loss: 0.7674 - acc: 0.6482\n",
      "Epoch 73/200\n",
      "3502/3502 [==============================] - 3s 956us/sample - loss: 0.7538 - acc: 0.6593\n",
      "Epoch 74/200\n",
      "3502/3502 [==============================] - 3s 895us/sample - loss: 0.7500 - acc: 0.6436\n",
      "Epoch 75/200\n",
      "3502/3502 [==============================] - 3s 787us/sample - loss: 0.7615 - acc: 0.6491\n",
      "Epoch 76/200\n",
      "3502/3502 [==============================] - 3s 727us/sample - loss: 0.7681 - acc: 0.6422\n",
      "Epoch 77/200\n",
      "3502/3502 [==============================] - 3s 736us/sample - loss: 0.7474 - acc: 0.6602\n",
      "Epoch 78/200\n",
      "3502/3502 [==============================] - 3s 798us/sample - loss: 0.7534 - acc: 0.6571\n",
      "Epoch 79/200\n",
      "3502/3502 [==============================] - 3s 905us/sample - loss: 0.7488 - acc: 0.6565\n",
      "Epoch 80/200\n",
      "3502/3502 [==============================] - 3s 727us/sample - loss: 0.7459 - acc: 0.6690\n",
      "Epoch 81/200\n",
      "3502/3502 [==============================] - 2s 664us/sample - loss: 0.7694 - acc: 0.6453\n",
      "Epoch 82/200\n",
      "3502/3502 [==============================] - 2s 651us/sample - loss: 0.7623 - acc: 0.6502\n",
      "Epoch 83/200\n",
      "3502/3502 [==============================] - 2s 646us/sample - loss: 0.7574 - acc: 0.6588\n",
      "Epoch 84/200\n",
      "3502/3502 [==============================] - 2s 649us/sample - loss: 0.7491 - acc: 0.6571\n",
      "Epoch 85/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.7478 - acc: 0.6588\n",
      "Epoch 86/200\n",
      "3502/3502 [==============================] - 2s 648us/sample - loss: 0.7530 - acc: 0.6485\n",
      "Epoch 87/200\n",
      "3502/3502 [==============================] - 2s 657us/sample - loss: 0.7570 - acc: 0.6551\n",
      "Epoch 88/200\n",
      "3502/3502 [==============================] - 2s 652us/sample - loss: 0.7392 - acc: 0.6611\n",
      "Epoch 89/200\n",
      "3502/3502 [==============================] - 2s 656us/sample - loss: 0.7481 - acc: 0.6542\n",
      "Epoch 90/200\n",
      "3502/3502 [==============================] - 2s 647us/sample - loss: 0.7505 - acc: 0.6528\n",
      "Epoch 91/200\n",
      "3502/3502 [==============================] - 2s 648us/sample - loss: 0.7591 - acc: 0.6639\n",
      "Epoch 92/200\n",
      "3502/3502 [==============================] - 2s 650us/sample - loss: 0.7399 - acc: 0.6591\n",
      "Epoch 93/200\n",
      "3502/3502 [==============================] - 2s 655us/sample - loss: 0.7871 - acc: 0.6416\n",
      "Epoch 94/200\n",
      "3502/3502 [==============================] - 2s 659us/sample - loss: 0.7672 - acc: 0.6556\n",
      "Epoch 95/200\n",
      "3502/3502 [==============================] - 2s 650us/sample - loss: 0.7518 - acc: 0.6513\n",
      "Epoch 96/200\n",
      "3502/3502 [==============================] - 2s 653us/sample - loss: 0.7425 - acc: 0.6622\n",
      "Epoch 97/200\n",
      "3502/3502 [==============================] - 3s 785us/sample - loss: 0.7440 - acc: 0.6599\n",
      "Epoch 98/200\n",
      "3502/3502 [==============================] - 2s 666us/sample - loss: 0.7470 - acc: 0.6559\n",
      "Epoch 99/200\n",
      "3502/3502 [==============================] - 2s 705us/sample - loss: 0.7372 - acc: 0.6593\n",
      "Epoch 100/200\n",
      "3502/3502 [==============================] - 2s 698us/sample - loss: 0.7482 - acc: 0.6539\n",
      "Epoch 101/200\n",
      "3502/3502 [==============================] - 2s 660us/sample - loss: 0.7454 - acc: 0.6613\n",
      "Epoch 102/200\n",
      "3502/3502 [==============================] - 2s 660us/sample - loss: 0.7536 - acc: 0.6528\n",
      "Epoch 103/200\n",
      "3502/3502 [==============================] - 2s 657us/sample - loss: 0.7464 - acc: 0.6619\n",
      "Epoch 104/200\n",
      "3502/3502 [==============================] - 2s 661us/sample - loss: 0.7439 - acc: 0.6573\n",
      "Epoch 105/200\n",
      "3502/3502 [==============================] - 2s 659us/sample - loss: 0.7375 - acc: 0.6648\n",
      "Epoch 106/200\n",
      "3502/3502 [==============================] - 3s 755us/sample - loss: 0.7645 - acc: 0.6528\n",
      "Epoch 107/200\n",
      "3502/3502 [==============================] - 2s 670us/sample - loss: 0.7366 - acc: 0.6648\n",
      "Epoch 108/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.7329 - acc: 0.6679\n",
      "Epoch 109/200\n",
      "3502/3502 [==============================] - 3s 731us/sample - loss: 0.7353 - acc: 0.6619\n",
      "Epoch 110/200\n",
      "3502/3502 [==============================] - 2s 703us/sample - loss: 0.7425 - acc: 0.6602\n",
      "Epoch 111/200\n",
      "3502/3502 [==============================] - 2s 670us/sample - loss: 0.7490 - acc: 0.6588\n",
      "Epoch 112/200\n",
      "3502/3502 [==============================] - 2s 670us/sample - loss: 0.7415 - acc: 0.6588\n",
      "Epoch 113/200\n",
      "3502/3502 [==============================] - 3s 723us/sample - loss: 0.7426 - acc: 0.6622\n",
      "Epoch 114/200\n",
      "3502/3502 [==============================] - 3s 891us/sample - loss: 0.7391 - acc: 0.6565\n",
      "Epoch 115/200\n",
      "3502/3502 [==============================] - 3s 800us/sample - loss: 0.7530 - acc: 0.6485\n",
      "Epoch 116/200\n",
      "3502/3502 [==============================] - 3s 757us/sample - loss: 0.7386 - acc: 0.6628\n",
      "Epoch 117/200\n",
      "3502/3502 [==============================] - 2s 684us/sample - loss: 0.7495 - acc: 0.6533\n",
      "Epoch 118/200\n",
      "3502/3502 [==============================] - 2s 667us/sample - loss: 0.7361 - acc: 0.6602\n",
      "Epoch 119/200\n",
      "3502/3502 [==============================] - 3s 729us/sample - loss: 0.7348 - acc: 0.6722\n",
      "Epoch 120/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.7410 - acc: 0.6673\n",
      "Epoch 121/200\n",
      "3502/3502 [==============================] - 3s 730us/sample - loss: 0.7384 - acc: 0.6665\n",
      "Epoch 122/200\n",
      "3502/3502 [==============================] - 2s 669us/sample - loss: 0.7378 - acc: 0.6542\n",
      "Epoch 123/200\n",
      "3502/3502 [==============================] - 3s 747us/sample - loss: 0.7468 - acc: 0.6725\n",
      "Epoch 124/200\n",
      "3502/3502 [==============================] - 2s 692us/sample - loss: 0.7335 - acc: 0.6662\n",
      "Epoch 125/200\n",
      "3502/3502 [==============================] - 3s 723us/sample - loss: 0.7391 - acc: 0.6619\n",
      "Epoch 126/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.7479 - acc: 0.6605\n",
      "Epoch 127/200\n",
      "3502/3502 [==============================] - 2s 696us/sample - loss: 0.7346 - acc: 0.6690\n",
      "Epoch 128/200\n",
      "3502/3502 [==============================] - 2s 674us/sample - loss: 0.7343 - acc: 0.6668\n",
      "Epoch 129/200\n",
      "3502/3502 [==============================] - 2s 677us/sample - loss: 0.7338 - acc: 0.6630\n",
      "Epoch 130/200\n",
      "3502/3502 [==============================] - 2s 673us/sample - loss: 0.7381 - acc: 0.6648\n",
      "Epoch 131/200\n",
      "3502/3502 [==============================] - 2s 673us/sample - loss: 0.7401 - acc: 0.6670\n",
      "Epoch 132/200\n",
      "3502/3502 [==============================] - 3s 904us/sample - loss: 0.7355 - acc: 0.6676\n",
      "Epoch 133/200\n",
      "3502/3502 [==============================] - 2s 676us/sample - loss: 0.7266 - acc: 0.6668\n",
      "Epoch 134/200\n",
      "3502/3502 [==============================] - 2s 673us/sample - loss: 0.7253 - acc: 0.6742\n",
      "Epoch 135/200\n",
      "3502/3502 [==============================] - 3s 752us/sample - loss: 0.7393 - acc: 0.6591\n",
      "Epoch 136/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.7552 - acc: 0.6508\n",
      "Epoch 137/200\n",
      "3502/3502 [==============================] - 2s 680us/sample - loss: 0.7422 - acc: 0.6630\n",
      "Epoch 138/200\n",
      "3502/3502 [==============================] - 2s 685us/sample - loss: 0.7355 - acc: 0.6633\n",
      "Epoch 139/200\n",
      "3502/3502 [==============================] - 2s 674us/sample - loss: 0.7359 - acc: 0.6673\n",
      "Epoch 140/200\n",
      "3502/3502 [==============================] - 3s 812us/sample - loss: 0.7463 - acc: 0.6516\n",
      "Epoch 141/200\n",
      "3502/3502 [==============================] - 2s 687us/sample - loss: 0.7459 - acc: 0.6528\n",
      "Epoch 142/200\n",
      "3502/3502 [==============================] - 3s 812us/sample - loss: 0.7369 - acc: 0.6625\n",
      "Epoch 143/200\n",
      "3502/3502 [==============================] - 3s 979us/sample - loss: 0.7507 - acc: 0.6439\n",
      "Epoch 144/200\n",
      "3502/3502 [==============================] - 3s 885us/sample - loss: 0.7344 - acc: 0.6593\n",
      "Epoch 145/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.7309 - acc: 0.6642\n",
      "Epoch 146/200\n",
      "3502/3502 [==============================] - 3s 716us/sample - loss: 0.7291 - acc: 0.6642\n",
      "Epoch 147/200\n",
      "3502/3502 [==============================] - 3s 755us/sample - loss: 0.7419 - acc: 0.6599\n",
      "Epoch 148/200\n",
      "3502/3502 [==============================] - 3s 724us/sample - loss: 0.7504 - acc: 0.6522\n",
      "Epoch 149/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.7230 - acc: 0.6708\n",
      "Epoch 150/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.7257 - acc: 0.6688\n",
      "Epoch 151/200\n",
      "3502/3502 [==============================] - 3s 723us/sample - loss: 0.7444 - acc: 0.6516\n",
      "Epoch 152/200\n",
      "3502/3502 [==============================] - 2s 690us/sample - loss: 0.7281 - acc: 0.6676\n",
      "Epoch 153/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.7266 - acc: 0.6628\n",
      "Epoch 154/200\n",
      "3502/3502 [==============================] - 2s 683us/sample - loss: 0.7341 - acc: 0.6622\n",
      "Epoch 155/200\n",
      "3502/3502 [==============================] - 2s 681us/sample - loss: 0.7430 - acc: 0.6593\n",
      "Epoch 156/200\n",
      "3502/3502 [==============================] - 2s 697us/sample - loss: 0.7361 - acc: 0.6628\n",
      "Epoch 157/200\n",
      "3502/3502 [==============================] - 2s 686us/sample - loss: 0.7449 - acc: 0.6545\n",
      "Epoch 158/200\n",
      "3502/3502 [==============================] - 2s 695us/sample - loss: 0.7288 - acc: 0.6676\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3502/3502 [==============================] - 2s 643us/sample - loss: 0.7332 - acc: 0.6636\n",
      "Epoch 160/200\n",
      "3502/3502 [==============================] - 2s 662us/sample - loss: 0.7333 - acc: 0.6679\n",
      "Epoch 161/200\n",
      "3502/3502 [==============================] - 2s 649us/sample - loss: 0.7351 - acc: 0.6591\n",
      "Epoch 162/200\n",
      "3502/3502 [==============================] - 2s 650us/sample - loss: 0.7363 - acc: 0.6619\n",
      "Epoch 163/200\n",
      "3502/3502 [==============================] - 2s 667us/sample - loss: 0.7333 - acc: 0.6653\n",
      "Epoch 164/200\n",
      "3502/3502 [==============================] - 3s 734us/sample - loss: 0.7362 - acc: 0.6650\n",
      "Epoch 165/200\n",
      "3502/3502 [==============================] - 2s 698us/sample - loss: 0.7404 - acc: 0.6628\n",
      "Epoch 166/200\n",
      "3502/3502 [==============================] - 2s 695us/sample - loss: 0.7390 - acc: 0.6611\n",
      "Epoch 167/200\n",
      "3502/3502 [==============================] - 2s 703us/sample - loss: 0.7318 - acc: 0.6673\n",
      "Epoch 168/200\n",
      "3502/3502 [==============================] - 2s 697us/sample - loss: 0.7441 - acc: 0.6591\n",
      "Epoch 169/200\n",
      "3502/3502 [==============================] - 3s 797us/sample - loss: 0.7352 - acc: 0.6556\n",
      "Epoch 170/200\n",
      "3502/3502 [==============================] - 3s 787us/sample - loss: 0.7323 - acc: 0.6605\n",
      "Epoch 171/200\n",
      "3502/3502 [==============================] - 3s 813us/sample - loss: 0.7197 - acc: 0.6679\n",
      "Epoch 172/200\n",
      "3502/3502 [==============================] - 3s 775us/sample - loss: 0.7400 - acc: 0.6622\n",
      "Epoch 173/200\n",
      "3502/3502 [==============================] - 3s 735us/sample - loss: 0.7317 - acc: 0.6628\n",
      "Epoch 174/200\n",
      "3502/3502 [==============================] - 3s 803us/sample - loss: 0.7353 - acc: 0.6650\n",
      "Epoch 175/200\n",
      "3502/3502 [==============================] - 3s 735us/sample - loss: 0.7400 - acc: 0.6568\n",
      "Epoch 176/200\n",
      "3502/3502 [==============================] - 3s 745us/sample - loss: 0.7353 - acc: 0.6662\n",
      "Epoch 177/200\n",
      "3502/3502 [==============================] - 2s 698us/sample - loss: 0.7268 - acc: 0.6665\n",
      "Epoch 178/200\n",
      "3502/3502 [==============================] - 3s 805us/sample - loss: 0.7313 - acc: 0.6628\n",
      "Epoch 179/200\n",
      "3502/3502 [==============================] - 2s 710us/sample - loss: 0.7302 - acc: 0.6630\n",
      "Epoch 180/200\n",
      "3502/3502 [==============================] - 3s 747us/sample - loss: 0.7359 - acc: 0.6611\n",
      "Epoch 181/200\n",
      "3502/3502 [==============================] - 2s 661us/sample - loss: 0.7426 - acc: 0.6685\n",
      "Epoch 182/200\n",
      "3502/3502 [==============================] - 2s 664us/sample - loss: 0.7264 - acc: 0.6745\n",
      "Epoch 183/200\n",
      "3502/3502 [==============================] - 2s 666us/sample - loss: 0.7334 - acc: 0.6650\n",
      "Epoch 184/200\n",
      "3502/3502 [==============================] - 2s 674us/sample - loss: 0.7223 - acc: 0.6702\n",
      "Epoch 185/200\n",
      "3502/3502 [==============================] - 3s 743us/sample - loss: 0.7284 - acc: 0.6645\n",
      "Epoch 186/200\n",
      "3502/3502 [==============================] - 3s 734us/sample - loss: 0.7278 - acc: 0.6611\n",
      "Epoch 187/200\n",
      "3502/3502 [==============================] - 2s 704us/sample - loss: 0.7344 - acc: 0.6656\n",
      "Epoch 188/200\n",
      "3502/3502 [==============================] - 3s 716us/sample - loss: 0.7231 - acc: 0.6602\n",
      "Epoch 189/200\n",
      "3502/3502 [==============================] - 2s 663us/sample - loss: 0.7312 - acc: 0.6633\n",
      "Epoch 190/200\n",
      "3502/3502 [==============================] - 2s 663us/sample - loss: 0.7364 - acc: 0.6593\n",
      "Epoch 191/200\n",
      "3502/3502 [==============================] - 2s 668us/sample - loss: 0.7366 - acc: 0.6596\n",
      "Epoch 192/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.7327 - acc: 0.6602\n",
      "Epoch 193/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.7327 - acc: 0.6690\n",
      "Epoch 194/200\n",
      "3502/3502 [==============================] - 3s 720us/sample - loss: 0.7223 - acc: 0.6679\n",
      "Epoch 195/200\n",
      "3502/3502 [==============================] - 2s 696us/sample - loss: 0.7315 - acc: 0.6653\n",
      "Epoch 196/200\n",
      "3502/3502 [==============================] - 2s 668us/sample - loss: 0.7289 - acc: 0.6650\n",
      "Epoch 197/200\n",
      "3502/3502 [==============================] - 2s 693us/sample - loss: 0.7304 - acc: 0.6630\n",
      "Epoch 198/200\n",
      "3502/3502 [==============================] - 2s 712us/sample - loss: 0.7305 - acc: 0.6591\n",
      "Epoch 199/200\n",
      "3502/3502 [==============================] - 2s 691us/sample - loss: 0.7241 - acc: 0.6705\n",
      "Epoch 200/200\n",
      "3502/3502 [==============================] - 2s 714us/sample - loss: 0.7305 - acc: 0.6611\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(256, input_shape=(1, 21), activation='relu', return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=200, batch_size=20, verbose=1)\n",
    "model.save('rnn_all_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66378164 0.7577571179196932\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, batch_size=20)\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> France 0.6347352\n",
    "<br> Spain 0.6623537\n",
    "<br> England 0.66789895\n",
    "<br> Germany 0.65761316\n",
    "<br> Italy 0.66378164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "football = football.drop(columns=['hTR'], axis=1)\n",
    "x,y = football.loc[:,football.columns != 'fTR'], football.loc[:,'fTR']\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\n",
    "\n",
    "\n",
    "x_train = x_train.astype(float)\n",
    "x_test = x_test.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3502/3502 [==============================] - 4s 1ms/sample - loss: 1.3834 - acc: 0.4146\n",
      "Epoch 2/200\n",
      "3502/3502 [==============================] - 2s 654us/sample - loss: 1.0634 - acc: 0.4792\n",
      "Epoch 3/200\n",
      "3502/3502 [==============================] - 3s 749us/sample - loss: 1.0064 - acc: 0.5054\n",
      "Epoch 4/200\n",
      "3502/3502 [==============================] - 3s 735us/sample - loss: 0.9889 - acc: 0.5171\n",
      "Epoch 5/200\n",
      "3502/3502 [==============================] - 2s 659us/sample - loss: 0.9536 - acc: 0.5380\n",
      "Epoch 6/200\n",
      "3502/3502 [==============================] - 2s 661us/sample - loss: 0.9777 - acc: 0.5420\n",
      "Epoch 7/200\n",
      "3502/3502 [==============================] - 2s 649us/sample - loss: 0.9429 - acc: 0.5397\n",
      "Epoch 8/200\n",
      "3502/3502 [==============================] - 2s 644us/sample - loss: 0.9131 - acc: 0.5597\n",
      "Epoch 9/200\n",
      "3502/3502 [==============================] - 2s 692us/sample - loss: 0.9449 - acc: 0.5600\n",
      "Epoch 10/200\n",
      "3502/3502 [==============================] - 3s 731us/sample - loss: 0.8968 - acc: 0.5851\n",
      "Epoch 11/200\n",
      "3502/3502 [==============================] - 3s 881us/sample - loss: 0.9060 - acc: 0.5697\n",
      "Epoch 12/200\n",
      "3502/3502 [==============================] - 3s 799us/sample - loss: 0.9057 - acc: 0.5785\n",
      "Epoch 13/200\n",
      "3502/3502 [==============================] - 3s 751us/sample - loss: 0.9368 - acc: 0.5585\n",
      "Epoch 14/200\n",
      "3502/3502 [==============================] - 3s 738us/sample - loss: 0.9014 - acc: 0.5742\n",
      "Epoch 15/200\n",
      "3502/3502 [==============================] - 3s 769us/sample - loss: 0.8877 - acc: 0.5751\n",
      "Epoch 16/200\n",
      "3502/3502 [==============================] - 3s 913us/sample - loss: 0.8997 - acc: 0.5722\n",
      "Epoch 17/200\n",
      "3502/3502 [==============================] - 2s 703us/sample - loss: 0.8882 - acc: 0.5897\n",
      "Epoch 18/200\n",
      "3502/3502 [==============================] - 2s 655us/sample - loss: 0.8709 - acc: 0.5919\n",
      "Epoch 19/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.8718 - acc: 0.5931\n",
      "Epoch 20/200\n",
      "3502/3502 [==============================] - 2s 668us/sample - loss: 0.8908 - acc: 0.5851\n",
      "Epoch 21/200\n",
      "3502/3502 [==============================] - 2s 686us/sample - loss: 0.8773 - acc: 0.5948\n",
      "Epoch 22/200\n",
      "3502/3502 [==============================] - 2s 702us/sample - loss: 0.8825 - acc: 0.5842\n",
      "Epoch 23/200\n",
      "3502/3502 [==============================] - 3s 719us/sample - loss: 0.8603 - acc: 0.5937\n",
      "Epoch 24/200\n",
      "3502/3502 [==============================] - 3s 782us/sample - loss: 0.8648 - acc: 0.5928\n",
      "Epoch 25/200\n",
      "3502/3502 [==============================] - 2s 713us/sample - loss: 0.8752 - acc: 0.5925\n",
      "Epoch 26/200\n",
      "3502/3502 [==============================] - 3s 766us/sample - loss: 0.8567 - acc: 0.6048\n",
      "Epoch 27/200\n",
      "3502/3502 [==============================] - 2s 701us/sample - loss: 0.8873 - acc: 0.5802\n",
      "Epoch 28/200\n",
      "3502/3502 [==============================] - 3s 733us/sample - loss: 0.8824 - acc: 0.5845\n",
      "Epoch 29/200\n",
      "3502/3502 [==============================] - 3s 722us/sample - loss: 0.8713 - acc: 0.5982\n",
      "Epoch 30/200\n",
      "3502/3502 [==============================] - 3s 809us/sample - loss: 0.8728 - acc: 0.5874\n",
      "Epoch 31/200\n",
      "3502/3502 [==============================] - 2s 710us/sample - loss: 0.8714 - acc: 0.5931\n",
      "Epoch 32/200\n",
      "3502/3502 [==============================] - 2s 698us/sample - loss: 0.8646 - acc: 0.5928\n",
      "Epoch 33/200\n",
      "3502/3502 [==============================] - 3s 854us/sample - loss: 0.8892 - acc: 0.5845\n",
      "Epoch 34/200\n",
      "3502/3502 [==============================] - 3s 876us/sample - loss: 0.8630 - acc: 0.6034\n",
      "Epoch 35/200\n",
      "3502/3502 [==============================] - 3s 744us/sample - loss: 0.8798 - acc: 0.5902\n",
      "Epoch 36/200\n",
      "3502/3502 [==============================] - 3s 789us/sample - loss: 0.8606 - acc: 0.5937\n",
      "Epoch 37/200\n",
      "3502/3502 [==============================] - 3s 758us/sample - loss: 0.8617 - acc: 0.5968\n",
      "Epoch 38/200\n",
      "3502/3502 [==============================] - 3s 792us/sample - loss: 0.8630 - acc: 0.5991\n",
      "Epoch 39/200\n",
      "3502/3502 [==============================] - 3s 718us/sample - loss: 0.8600 - acc: 0.6051\n",
      "Epoch 40/200\n",
      "3502/3502 [==============================] - 3s 743us/sample - loss: 0.8825 - acc: 0.5857\n",
      "Epoch 41/200\n",
      "3502/3502 [==============================] - 3s 807us/sample - loss: 0.8693 - acc: 0.5962\n",
      "Epoch 42/200\n",
      "3502/3502 [==============================] - 2s 660us/sample - loss: 0.8599 - acc: 0.5962\n",
      "Epoch 43/200\n",
      "3502/3502 [==============================] - 2s 709us/sample - loss: 0.8689 - acc: 0.5994\n",
      "Epoch 44/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.8643 - acc: 0.5931\n",
      "Epoch 45/200\n",
      "3502/3502 [==============================] - 3s 734us/sample - loss: 0.8671 - acc: 0.5962\n",
      "Epoch 46/200\n",
      "3502/3502 [==============================] - 2s 663us/sample - loss: 0.8631 - acc: 0.5977\n",
      "Epoch 47/200\n",
      "3502/3502 [==============================] - 3s 731us/sample - loss: 0.8569 - acc: 0.5959\n",
      "Epoch 48/200\n",
      "3502/3502 [==============================] - 2s 677us/sample - loss: 0.8644 - acc: 0.6039\n",
      "Epoch 49/200\n",
      "3502/3502 [==============================] - 2s 669us/sample - loss: 0.8694 - acc: 0.5988\n",
      "Epoch 50/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.8536 - acc: 0.6019\n",
      "Epoch 51/200\n",
      "3502/3502 [==============================] - 3s 715us/sample - loss: 0.8611 - acc: 0.5991\n",
      "Epoch 52/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.8734 - acc: 0.5854\n",
      "Epoch 53/200\n",
      "3502/3502 [==============================] - 2s 685us/sample - loss: 0.8576 - acc: 0.6031\n",
      "Epoch 54/200\n",
      "3502/3502 [==============================] - 2s 691us/sample - loss: 0.8650 - acc: 0.5974\n",
      "Epoch 55/200\n",
      "3502/3502 [==============================] - 2s 667us/sample - loss: 0.8478 - acc: 0.6105\n",
      "Epoch 56/200\n",
      "3502/3502 [==============================] - 2s 664us/sample - loss: 0.8620 - acc: 0.5957\n",
      "Epoch 57/200\n",
      "3502/3502 [==============================] - 3s 829us/sample - loss: 0.8559 - acc: 0.6002\n",
      "Epoch 58/200\n",
      "3502/3502 [==============================] - 2s 707us/sample - loss: 0.8730 - acc: 0.5820\n",
      "Epoch 59/200\n",
      "3502/3502 [==============================] - 3s 808us/sample - loss: 0.8585 - acc: 0.5991\n",
      "Epoch 60/200\n",
      "3502/3502 [==============================] - 3s 739us/sample - loss: 0.8597 - acc: 0.5991\n",
      "Epoch 61/200\n",
      "3502/3502 [==============================] - 3s 737us/sample - loss: 0.8616 - acc: 0.5979\n",
      "Epoch 62/200\n",
      "3502/3502 [==============================] - 3s 789us/sample - loss: 0.8517 - acc: 0.6011\n",
      "Epoch 63/200\n",
      "3502/3502 [==============================] - 3s 739us/sample - loss: 0.8520 - acc: 0.6079\n",
      "Epoch 64/200\n",
      "3502/3502 [==============================] - 3s 790us/sample - loss: 0.8572 - acc: 0.5982\n",
      "Epoch 65/200\n",
      "3502/3502 [==============================] - 3s 814us/sample - loss: 0.8521 - acc: 0.6102\n",
      "Epoch 66/200\n",
      "3502/3502 [==============================] - 3s 723us/sample - loss: 0.8560 - acc: 0.6025\n",
      "Epoch 67/200\n",
      "3502/3502 [==============================] - 2s 709us/sample - loss: 0.8495 - acc: 0.6028\n",
      "Epoch 68/200\n",
      "3502/3502 [==============================] - 3s 715us/sample - loss: 0.8489 - acc: 0.6065\n",
      "Epoch 69/200\n",
      "3502/3502 [==============================] - 3s 726us/sample - loss: 0.8516 - acc: 0.6102\n",
      "Epoch 70/200\n",
      "3502/3502 [==============================] - 2s 713us/sample - loss: 0.8483 - acc: 0.6054\n",
      "Epoch 71/200\n",
      "3502/3502 [==============================] - 3s 789us/sample - loss: 0.8485 - acc: 0.6131\n",
      "Epoch 72/200\n",
      "3502/3502 [==============================] - 2s 697us/sample - loss: 0.8456 - acc: 0.6045\n",
      "Epoch 73/200\n",
      "3502/3502 [==============================] - 3s 719us/sample - loss: 0.8457 - acc: 0.6105\n",
      "Epoch 74/200\n",
      "3502/3502 [==============================] - 3s 765us/sample - loss: 0.8493 - acc: 0.6057\n",
      "Epoch 75/200\n",
      "3502/3502 [==============================] - 2s 671us/sample - loss: 0.8432 - acc: 0.6097\n",
      "Epoch 76/200\n",
      "3502/3502 [==============================] - 2s 674us/sample - loss: 0.8423 - acc: 0.6091\n",
      "Epoch 77/200\n",
      "3502/3502 [==============================] - 2s 680us/sample - loss: 0.8459 - acc: 0.6022\n",
      "Epoch 78/200\n",
      "3502/3502 [==============================] - 2s 671us/sample - loss: 0.8552 - acc: 0.5962\n",
      "Epoch 79/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.8488 - acc: 0.6131\n",
      "Epoch 80/200\n",
      "3502/3502 [==============================] - 2s 636us/sample - loss: 0.8490 - acc: 0.6091\n",
      "Epoch 81/200\n",
      "3502/3502 [==============================] - 2s 634us/sample - loss: 0.8600 - acc: 0.6051\n",
      "Epoch 82/200\n",
      "3502/3502 [==============================] - 2s 673us/sample - loss: 0.8420 - acc: 0.6125\n",
      "Epoch 83/200\n",
      "3502/3502 [==============================] - 2s 662us/sample - loss: 0.8506 - acc: 0.6114\n",
      "Epoch 84/200\n",
      "3502/3502 [==============================] - 2s 643us/sample - loss: 0.8500 - acc: 0.6068\n",
      "Epoch 85/200\n",
      "3502/3502 [==============================] - 2s 639us/sample - loss: 0.8403 - acc: 0.6139\n",
      "Epoch 86/200\n",
      "3502/3502 [==============================] - 2s 640us/sample - loss: 0.8420 - acc: 0.6094\n",
      "Epoch 87/200\n",
      "3502/3502 [==============================] - 2s 641us/sample - loss: 0.8505 - acc: 0.6034\n",
      "Epoch 88/200\n",
      "3502/3502 [==============================] - 2s 639us/sample - loss: 0.8471 - acc: 0.6022\n",
      "Epoch 89/200\n",
      "3502/3502 [==============================] - 2s 645us/sample - loss: 0.8489 - acc: 0.6028\n",
      "Epoch 90/200\n",
      "3502/3502 [==============================] - 2s 639us/sample - loss: 0.8457 - acc: 0.6094\n",
      "Epoch 91/200\n",
      "3502/3502 [==============================] - 2s 645us/sample - loss: 0.8437 - acc: 0.6131\n",
      "Epoch 92/200\n",
      "3502/3502 [==============================] - 2s 645us/sample - loss: 0.8491 - acc: 0.6077\n",
      "Epoch 93/200\n",
      "3502/3502 [==============================] - 2s 646us/sample - loss: 0.8407 - acc: 0.6085\n",
      "Epoch 94/200\n",
      "3502/3502 [==============================] - 2s 648us/sample - loss: 0.8423 - acc: 0.6142\n",
      "Epoch 95/200\n",
      "3502/3502 [==============================] - 2s 646us/sample - loss: 0.8583 - acc: 0.6028\n",
      "Epoch 96/200\n",
      "3502/3502 [==============================] - 2s 654us/sample - loss: 0.8404 - acc: 0.6162\n",
      "Epoch 97/200\n",
      "3502/3502 [==============================] - 2s 651us/sample - loss: 0.8371 - acc: 0.6142\n",
      "Epoch 98/200\n",
      "3502/3502 [==============================] - 2s 640us/sample - loss: 0.8419 - acc: 0.6079\n",
      "Epoch 99/200\n",
      "3502/3502 [==============================] - 2s 690us/sample - loss: 0.8486 - acc: 0.6034\n",
      "Epoch 100/200\n",
      "3502/3502 [==============================] - 2s 646us/sample - loss: 0.8393 - acc: 0.6151\n",
      "Epoch 101/200\n",
      "3502/3502 [==============================] - 2s 652us/sample - loss: 0.8425 - acc: 0.6119\n",
      "Epoch 102/200\n",
      "3502/3502 [==============================] - 2s 649us/sample - loss: 0.8356 - acc: 0.6159\n",
      "Epoch 103/200\n",
      "3502/3502 [==============================] - 2s 647us/sample - loss: 0.8474 - acc: 0.6077\n",
      "Epoch 104/200\n",
      "3502/3502 [==============================] - 2s 650us/sample - loss: 0.8400 - acc: 0.6065\n",
      "Epoch 105/200\n",
      "3502/3502 [==============================] - 2s 650us/sample - loss: 0.8406 - acc: 0.6128\n",
      "Epoch 106/200\n",
      "3502/3502 [==============================] - 2s 656us/sample - loss: 0.8391 - acc: 0.6094\n",
      "Epoch 107/200\n",
      "3502/3502 [==============================] - 2s 654us/sample - loss: 0.8381 - acc: 0.6128\n",
      "Epoch 108/200\n",
      "3502/3502 [==============================] - 2s 648us/sample - loss: 0.8438 - acc: 0.6091\n",
      "Epoch 109/200\n",
      "3502/3502 [==============================] - 2s 655us/sample - loss: 0.8432 - acc: 0.6122\n",
      "Epoch 110/200\n",
      "3502/3502 [==============================] - 2s 654us/sample - loss: 0.8359 - acc: 0.6094\n",
      "Epoch 111/200\n",
      "3502/3502 [==============================] - 2s 653us/sample - loss: 0.8451 - acc: 0.6028\n",
      "Epoch 112/200\n",
      "3502/3502 [==============================] - 3s 729us/sample - loss: 0.8398 - acc: 0.6108\n",
      "Epoch 113/200\n",
      "3502/3502 [==============================] - 2s 712us/sample - loss: 0.8476 - acc: 0.6154\n",
      "Epoch 114/200\n",
      "3502/3502 [==============================] - 3s 856us/sample - loss: 0.8328 - acc: 0.6211\n",
      "Epoch 115/200\n",
      "3502/3502 [==============================] - 2s 699us/sample - loss: 0.8373 - acc: 0.6165\n",
      "Epoch 116/200\n",
      "3502/3502 [==============================] - 3s 748us/sample - loss: 0.8323 - acc: 0.6162\n",
      "Epoch 117/200\n",
      "3502/3502 [==============================] - 3s 743us/sample - loss: 0.8325 - acc: 0.6185\n",
      "Epoch 118/200\n",
      "3502/3502 [==============================] - 2s 671us/sample - loss: 0.8397 - acc: 0.6071\n",
      "Epoch 119/200\n",
      "3502/3502 [==============================] - 2s 693us/sample - loss: 0.8410 - acc: 0.6082\n",
      "Epoch 120/200\n",
      "3502/3502 [==============================] - 2s 662us/sample - loss: 0.8387 - acc: 0.6105\n",
      "Epoch 121/200\n",
      "3502/3502 [==============================] - 2s 668us/sample - loss: 0.8329 - acc: 0.6159\n",
      "Epoch 122/200\n",
      "3502/3502 [==============================] - 2s 666us/sample - loss: 0.8343 - acc: 0.6194\n",
      "Epoch 123/200\n",
      "3502/3502 [==============================] - 2s 666us/sample - loss: 0.8389 - acc: 0.6071\n",
      "Epoch 124/200\n",
      "3502/3502 [==============================] - 2s 653us/sample - loss: 0.8449 - acc: 0.6051\n",
      "Epoch 125/200\n",
      "3502/3502 [==============================] - 2s 674us/sample - loss: 0.8301 - acc: 0.6196\n",
      "Epoch 126/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.8274 - acc: 0.6165\n",
      "Epoch 127/200\n",
      "3502/3502 [==============================] - 2s 668us/sample - loss: 0.8354 - acc: 0.6099\n",
      "Epoch 128/200\n",
      "3502/3502 [==============================] - 2s 669us/sample - loss: 0.8336 - acc: 0.6131\n",
      "Epoch 129/200\n",
      "3502/3502 [==============================] - 2s 661us/sample - loss: 0.8257 - acc: 0.6219\n",
      "Epoch 130/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.8288 - acc: 0.6242\n",
      "Epoch 131/200\n",
      "3502/3502 [==============================] - 3s 785us/sample - loss: 0.8271 - acc: 0.6199\n",
      "Epoch 132/200\n",
      "3502/3502 [==============================] - 3s 935us/sample - loss: 0.8382 - acc: 0.6099\n",
      "Epoch 133/200\n",
      "3502/3502 [==============================] - 3s 895us/sample - loss: 0.8277 - acc: 0.6171\n",
      "Epoch 134/200\n",
      "3502/3502 [==============================] - 3s 764us/sample - loss: 0.8335 - acc: 0.6156\n",
      "Epoch 135/200\n",
      "3502/3502 [==============================] - 3s 801us/sample - loss: 0.8233 - acc: 0.6179\n",
      "Epoch 136/200\n",
      "3502/3502 [==============================] - 3s 738us/sample - loss: 0.8324 - acc: 0.6131\n",
      "Epoch 137/200\n",
      "3502/3502 [==============================] - 2s 695us/sample - loss: 0.8283 - acc: 0.6194\n",
      "Epoch 138/200\n",
      "3502/3502 [==============================] - 2s 676us/sample - loss: 0.8304 - acc: 0.6108\n",
      "Epoch 139/200\n",
      "3502/3502 [==============================] - 3s 723us/sample - loss: 0.8352 - acc: 0.6139\n",
      "Epoch 140/200\n",
      "3502/3502 [==============================] - 2s 699us/sample - loss: 0.8327 - acc: 0.6202\n",
      "Epoch 141/200\n",
      "3502/3502 [==============================] - 2s 673us/sample - loss: 0.8317 - acc: 0.6145\n",
      "Epoch 142/200\n",
      "3502/3502 [==============================] - 2s 693us/sample - loss: 0.8437 - acc: 0.6054\n",
      "Epoch 143/200\n",
      "3502/3502 [==============================] - 2s 672us/sample - loss: 0.8342 - acc: 0.6128\n",
      "Epoch 144/200\n",
      "3502/3502 [==============================] - 2s 674us/sample - loss: 0.8241 - acc: 0.6222\n",
      "Epoch 145/200\n",
      "3502/3502 [==============================] - 3s 716us/sample - loss: 0.8265 - acc: 0.6216\n",
      "Epoch 146/200\n",
      "3502/3502 [==============================] - 2s 687us/sample - loss: 0.8330 - acc: 0.6136\n",
      "Epoch 147/200\n",
      "3502/3502 [==============================] - 2s 676us/sample - loss: 0.8258 - acc: 0.6179\n",
      "Epoch 148/200\n",
      "3502/3502 [==============================] - 2s 683us/sample - loss: 0.8288 - acc: 0.6114\n",
      "Epoch 149/200\n",
      "3502/3502 [==============================] - 2s 676us/sample - loss: 0.8221 - acc: 0.6219\n",
      "Epoch 150/200\n",
      "3502/3502 [==============================] - 2s 694us/sample - loss: 0.8257 - acc: 0.6194\n",
      "Epoch 151/200\n",
      "3502/3502 [==============================] - 2s 676us/sample - loss: 0.8302 - acc: 0.6125\n",
      "Epoch 152/200\n",
      "3502/3502 [==============================] - 2s 680us/sample - loss: 0.8353 - acc: 0.6156\n",
      "Epoch 153/200\n",
      "3502/3502 [==============================] - 2s 693us/sample - loss: 0.8276 - acc: 0.6162\n",
      "Epoch 154/200\n",
      "3502/3502 [==============================] - 3s 726us/sample - loss: 0.8240 - acc: 0.6191\n",
      "Epoch 155/200\n",
      "3502/3502 [==============================] - 2s 687us/sample - loss: 0.8282 - acc: 0.6219\n",
      "Epoch 156/200\n",
      "3502/3502 [==============================] - 2s 695us/sample - loss: 0.8191 - acc: 0.6225\n",
      "Epoch 157/200\n",
      "3502/3502 [==============================] - 2s 675us/sample - loss: 0.8304 - acc: 0.6202\n",
      "Epoch 158/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.8292 - acc: 0.6194\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3502/3502 [==============================] - 2s 637us/sample - loss: 0.8331 - acc: 0.6108\n",
      "Epoch 160/200\n",
      "3502/3502 [==============================] - 2s 644us/sample - loss: 0.8216 - acc: 0.6222\n",
      "Epoch 161/200\n",
      "3502/3502 [==============================] - 2s 641us/sample - loss: 0.8282 - acc: 0.6159\n",
      "Epoch 162/200\n",
      "3502/3502 [==============================] - 2s 640us/sample - loss: 0.8275 - acc: 0.6125\n",
      "Epoch 163/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.8201 - acc: 0.6231\n",
      "Epoch 164/200\n",
      "3502/3502 [==============================] - 2s 674us/sample - loss: 0.8206 - acc: 0.6171\n",
      "Epoch 165/200\n",
      "3502/3502 [==============================] - 2s 676us/sample - loss: 0.8221 - acc: 0.6156\n",
      "Epoch 166/200\n",
      "3502/3502 [==============================] - 3s 807us/sample - loss: 0.8320 - acc: 0.6117\n",
      "Epoch 167/200\n",
      "3502/3502 [==============================] - 2s 713us/sample - loss: 0.8289 - acc: 0.6176\n",
      "Epoch 168/200\n",
      "3502/3502 [==============================] - 2s 705us/sample - loss: 0.8276 - acc: 0.6131\n",
      "Epoch 169/200\n",
      "3502/3502 [==============================] - 2s 701us/sample - loss: 0.8270 - acc: 0.6151\n",
      "Epoch 170/200\n",
      "3502/3502 [==============================] - 2s 686us/sample - loss: 0.8167 - acc: 0.6242\n",
      "Epoch 171/200\n",
      "3502/3502 [==============================] - 2s 698us/sample - loss: 0.8223 - acc: 0.6225\n",
      "Epoch 172/200\n",
      "3502/3502 [==============================] - 3s 734us/sample - loss: 0.8195 - acc: 0.6188\n",
      "Epoch 173/200\n",
      "3502/3502 [==============================] - 3s 748us/sample - loss: 0.8200 - acc: 0.6208\n",
      "Epoch 174/200\n",
      "3502/3502 [==============================] - 3s 807us/sample - loss: 0.8114 - acc: 0.6225\n",
      "Epoch 175/200\n",
      "3502/3502 [==============================] - 3s 758us/sample - loss: 0.8209 - acc: 0.6214\n",
      "Epoch 176/200\n",
      "3502/3502 [==============================] - 3s 784us/sample - loss: 0.8274 - acc: 0.6188\n",
      "Epoch 177/200\n",
      "3502/3502 [==============================] - 2s 691us/sample - loss: 0.8216 - acc: 0.6236\n",
      "Epoch 178/200\n",
      "3502/3502 [==============================] - 2s 650us/sample - loss: 0.8206 - acc: 0.6191\n",
      "Epoch 179/200\n",
      "3502/3502 [==============================] - 2s 699us/sample - loss: 0.8202 - acc: 0.6248\n",
      "Epoch 180/200\n",
      "3502/3502 [==============================] - 2s 645us/sample - loss: 0.8289 - acc: 0.6119\n",
      "Epoch 181/200\n",
      "3502/3502 [==============================] - 2s 657us/sample - loss: 0.8195 - acc: 0.6211\n",
      "Epoch 182/200\n",
      "3502/3502 [==============================] - 2s 647us/sample - loss: 0.8146 - acc: 0.6239\n",
      "Epoch 183/200\n",
      "3502/3502 [==============================] - 2s 678us/sample - loss: 0.8189 - acc: 0.6271\n",
      "Epoch 184/200\n",
      "3502/3502 [==============================] - 2s 648us/sample - loss: 0.8269 - acc: 0.6168\n",
      "Epoch 185/200\n",
      "3502/3502 [==============================] - 2s 651us/sample - loss: 0.8226 - acc: 0.6191\n",
      "Epoch 186/200\n",
      "3502/3502 [==============================] - 2s 666us/sample - loss: 0.8176 - acc: 0.6219\n",
      "Epoch 187/200\n",
      "3502/3502 [==============================] - 2s 658us/sample - loss: 0.8228 - acc: 0.6231\n",
      "Epoch 188/200\n",
      "3502/3502 [==============================] - 2s 676us/sample - loss: 0.8107 - acc: 0.6216\n",
      "Epoch 189/200\n",
      "3502/3502 [==============================] - 2s 661us/sample - loss: 0.8338 - acc: 0.6074\n",
      "Epoch 190/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.8213 - acc: 0.6256\n",
      "Epoch 191/200\n",
      "3502/3502 [==============================] - 2s 655us/sample - loss: 0.8197 - acc: 0.6279\n",
      "Epoch 192/200\n",
      "3502/3502 [==============================] - 2s 655us/sample - loss: 0.8154 - acc: 0.6319\n",
      "Epoch 193/200\n",
      "3502/3502 [==============================] - 2s 665us/sample - loss: 0.8220 - acc: 0.6251\n",
      "Epoch 194/200\n",
      "3502/3502 [==============================] - 2s 672us/sample - loss: 0.8167 - acc: 0.6239\n",
      "Epoch 195/200\n",
      "3502/3502 [==============================] - 3s 746us/sample - loss: 0.8174 - acc: 0.6234\n",
      "Epoch 196/200\n",
      "3502/3502 [==============================] - 2s 682us/sample - loss: 0.8259 - acc: 0.6182\n",
      "Epoch 197/200\n",
      "3502/3502 [==============================] - 2s 662us/sample - loss: 0.8144 - acc: 0.6219\n",
      "Epoch 198/200\n",
      "3502/3502 [==============================] - 2s 660us/sample - loss: 0.8166 - acc: 0.6176\n",
      "Epoch 199/200\n",
      "3502/3502 [==============================] - 2s 657us/sample - loss: 0.8175 - acc: 0.6148\n",
      "Epoch 200/200\n",
      "3502/3502 [==============================] - 2s 669us/sample - loss: 0.8115 - acc: 0.6225\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_no_htr = tf.keras.Sequential()\n",
    "model_no_htr.add(tf.keras.layers.LSTM(256, input_shape=(1, 20), activation='relu', return_sequences=True))\n",
    "model_no_htr.add(tf.keras.layers.LSTM(128, activation='relu'))\n",
    "model_no_htr.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "model_no_htr.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_no_htr.fit(x_train, y_train, epochs=200, batch_size=20, verbose=1)\n",
    "model_no_htr.save('rnn_no_htr_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5985353 0.8756807754423901\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_no_htr.predict(x_test, batch_size=20)\n",
    "val_loss, val_acc = model_no_htr.evaluate(x_test, y_test, verbose=0)\n",
    "print(val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> France 0.57476634\n",
    "<br> Spain 0.6155268\n",
    "<br> England 0.5902649\n",
    "<br> Germany 0.5761317\n",
    "<br> Italy 0.5985353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "football = football.drop(columns=['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR'], axis=1)\n",
    "x,y = football.loc[:,football.columns != 'fTR'], football.loc[:,'fTR']\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\n",
    "\n",
    "\n",
    "x_train = x_train.astype(float)\n",
    "x_test = x_test.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1124 22:01:01.955901 139867182622528 deprecation.py:506] From /home/kub/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1124 22:01:02.684263 139867182622528 deprecation.py:323] From /home/kub/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3502/3502 [==============================] - 8s 2ms/sample - loss: 1.4982 - acc: 0.4012\n",
      "Epoch 2/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 1.1336 - acc: 0.4189\n",
      "Epoch 3/200\n",
      "3502/3502 [==============================] - 5s 2ms/sample - loss: 1.1052 - acc: 0.4320\n",
      "Epoch 4/200\n",
      "3502/3502 [==============================] - 5s 2ms/sample - loss: 1.0659 - acc: 0.4517\n",
      "Epoch 5/200\n",
      "3502/3502 [==============================] - 5s 2ms/sample - loss: 1.0593 - acc: 0.4634\n",
      "Epoch 6/200\n",
      "3502/3502 [==============================] - 5s 2ms/sample - loss: 1.0461 - acc: 0.4692\n",
      "Epoch 7/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 1.0292 - acc: 0.4780\n",
      "Epoch 8/200\n",
      "3502/3502 [==============================] - 5s 2ms/sample - loss: 0.9991 - acc: 0.5003\n",
      "Epoch 9/200\n",
      "3502/3502 [==============================] - 5s 2ms/sample - loss: 1.0035 - acc: 0.4980\n",
      "Epoch 10/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 1.0053 - acc: 0.5137\n",
      "Epoch 11/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9940 - acc: 0.5208\n",
      "Epoch 12/200\n",
      "3502/3502 [==============================] - 5s 2ms/sample - loss: 0.9952 - acc: 0.5140\n",
      "Epoch 13/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9936 - acc: 0.5131\n",
      "Epoch 14/200\n",
      "3502/3502 [==============================] - 5s 2ms/sample - loss: 0.9820 - acc: 0.5137\n",
      "Epoch 15/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9886 - acc: 0.5160\n",
      "Epoch 16/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9764 - acc: 0.5280\n",
      "Epoch 17/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9725 - acc: 0.5206\n",
      "Epoch 18/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9846 - acc: 0.5223\n",
      "Epoch 19/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9753 - acc: 0.5260\n",
      "Epoch 20/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9873 - acc: 0.5166\n",
      "Epoch 21/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9800 - acc: 0.5240\n",
      "Epoch 22/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9737 - acc: 0.5240\n",
      "Epoch 23/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9609 - acc: 0.5417\n",
      "Epoch 24/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9793 - acc: 0.5291\n",
      "Epoch 25/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9785 - acc: 0.5248\n",
      "Epoch 26/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9663 - acc: 0.5288\n",
      "Epoch 27/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9684 - acc: 0.5271\n",
      "Epoch 28/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9674 - acc: 0.5326\n",
      "Epoch 29/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9683 - acc: 0.5291\n",
      "Epoch 30/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9693 - acc: 0.5334\n",
      "Epoch 31/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9634 - acc: 0.5311\n",
      "Epoch 32/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9597 - acc: 0.5306\n",
      "Epoch 33/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9642 - acc: 0.5385\n",
      "Epoch 34/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9684 - acc: 0.5237\n",
      "Epoch 35/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9619 - acc: 0.5377\n",
      "Epoch 36/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9699 - acc: 0.5246\n",
      "Epoch 37/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9687 - acc: 0.5300\n",
      "Epoch 38/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9621 - acc: 0.5351\n",
      "Epoch 39/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9605 - acc: 0.5340 0s - loss: 0.9612\n",
      "Epoch 40/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9602 - acc: 0.5388\n",
      "Epoch 41/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9654 - acc: 0.5326\n",
      "Epoch 42/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9664 - acc: 0.5308\n",
      "Epoch 43/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9634 - acc: 0.5354\n",
      "Epoch 44/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9644 - acc: 0.5286\n",
      "Epoch 45/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9716 - acc: 0.5214\n",
      "Epoch 46/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9741 - acc: 0.5220\n",
      "Epoch 47/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9616 - acc: 0.5380\n",
      "Epoch 48/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9602 - acc: 0.5391\n",
      "Epoch 49/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9619 - acc: 0.5274\n",
      "Epoch 50/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9596 - acc: 0.5348\n",
      "Epoch 51/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9604 - acc: 0.5314\n",
      "Epoch 52/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9599 - acc: 0.5357\n",
      "Epoch 53/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9599 - acc: 0.5320\n",
      "Epoch 54/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9601 - acc: 0.5343\n",
      "Epoch 55/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9607 - acc: 0.5348\n",
      "Epoch 56/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9607 - acc: 0.5394 1s \n",
      "Epoch 57/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9621 - acc: 0.5374\n",
      "Epoch 58/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9672 - acc: 0.5348\n",
      "Epoch 59/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9614 - acc: 0.5334\n",
      "Epoch 60/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9591 - acc: 0.5314\n",
      "Epoch 61/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9595 - acc: 0.5346\n",
      "Epoch 62/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9595 - acc: 0.5348\n",
      "Epoch 63/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9591 - acc: 0.5323\n",
      "Epoch 64/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9589 - acc: 0.5308\n",
      "Epoch 65/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9632 - acc: 0.5271\n",
      "Epoch 66/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9656 - acc: 0.5246\n",
      "Epoch 67/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9635 - acc: 0.5317\n",
      "Epoch 68/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9579 - acc: 0.5371\n",
      "Epoch 69/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9584 - acc: 0.5368\n",
      "Epoch 70/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9573 - acc: 0.5357\n",
      "Epoch 71/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9558 - acc: 0.5374\n",
      "Epoch 72/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9594 - acc: 0.5340\n",
      "Epoch 73/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9569 - acc: 0.5425\n",
      "Epoch 74/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9642 - acc: 0.5283\n",
      "Epoch 75/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9599 - acc: 0.5340\n",
      "Epoch 76/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9569 - acc: 0.5343\n",
      "Epoch 77/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9607 - acc: 0.5331\n",
      "Epoch 78/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9553 - acc: 0.5411\n",
      "Epoch 79/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9593 - acc: 0.5366\n",
      "Epoch 80/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9591 - acc: 0.5377\n",
      "Epoch 81/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9592 - acc: 0.5368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9607 - acc: 0.5303\n",
      "Epoch 83/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9588 - acc: 0.5357\n",
      "Epoch 84/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9616 - acc: 0.5360\n",
      "Epoch 85/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9586 - acc: 0.5337\n",
      "Epoch 86/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9574 - acc: 0.5366\n",
      "Epoch 87/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9585 - acc: 0.5337\n",
      "Epoch 88/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9601 - acc: 0.5391\n",
      "Epoch 89/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9569 - acc: 0.5354\n",
      "Epoch 90/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9601 - acc: 0.5340\n",
      "Epoch 91/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9592 - acc: 0.5346\n",
      "Epoch 92/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9573 - acc: 0.5374\n",
      "Epoch 93/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9574 - acc: 0.5420\n",
      "Epoch 94/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9559 - acc: 0.5377\n",
      "Epoch 95/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9603 - acc: 0.5266\n",
      "Epoch 96/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9599 - acc: 0.5337\n",
      "Epoch 97/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9589 - acc: 0.5328\n",
      "Epoch 98/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9632 - acc: 0.5340\n",
      "Epoch 99/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9574 - acc: 0.5385\n",
      "Epoch 100/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9582 - acc: 0.5437\n",
      "Epoch 101/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9587 - acc: 0.5340\n",
      "Epoch 102/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9547 - acc: 0.5383\n",
      "Epoch 103/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9594 - acc: 0.5374\n",
      "Epoch 104/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9553 - acc: 0.5368\n",
      "Epoch 105/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9597 - acc: 0.5346\n",
      "Epoch 106/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9570 - acc: 0.5354\n",
      "Epoch 107/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9590 - acc: 0.5343\n",
      "Epoch 108/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9563 - acc: 0.5388\n",
      "Epoch 109/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9583 - acc: 0.5374\n",
      "Epoch 110/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9581 - acc: 0.5351\n",
      "Epoch 111/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9563 - acc: 0.5391\n",
      "Epoch 112/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9562 - acc: 0.5354\n",
      "Epoch 113/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9548 - acc: 0.5445\n",
      "Epoch 114/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9548 - acc: 0.5414\n",
      "Epoch 115/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9539 - acc: 0.5394\n",
      "Epoch 116/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9620 - acc: 0.5371\n",
      "Epoch 117/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9574 - acc: 0.5348\n",
      "Epoch 118/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9622 - acc: 0.5306\n",
      "Epoch 119/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9574 - acc: 0.5328\n",
      "Epoch 120/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9578 - acc: 0.5328\n",
      "Epoch 121/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9604 - acc: 0.5297\n",
      "Epoch 122/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9558 - acc: 0.5394\n",
      "Epoch 123/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9544 - acc: 0.5403\n",
      "Epoch 124/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9581 - acc: 0.5385\n",
      "Epoch 125/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9577 - acc: 0.5368\n",
      "Epoch 126/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9555 - acc: 0.5351\n",
      "Epoch 127/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9578 - acc: 0.5374\n",
      "Epoch 128/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9572 - acc: 0.5348\n",
      "Epoch 129/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9569 - acc: 0.5354\n",
      "Epoch 130/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9540 - acc: 0.5397\n",
      "Epoch 131/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9609 - acc: 0.5346\n",
      "Epoch 132/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9572 - acc: 0.5420\n",
      "Epoch 133/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9598 - acc: 0.5331\n",
      "Epoch 134/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9583 - acc: 0.5311\n",
      "Epoch 135/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9562 - acc: 0.5343\n",
      "Epoch 136/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9543 - acc: 0.5408\n",
      "Epoch 137/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9558 - acc: 0.5343\n",
      "Epoch 138/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9566 - acc: 0.5405\n",
      "Epoch 139/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9594 - acc: 0.5388\n",
      "Epoch 140/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9563 - acc: 0.5388\n",
      "Epoch 141/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9557 - acc: 0.5334\n",
      "Epoch 142/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9564 - acc: 0.5383\n",
      "Epoch 143/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9550 - acc: 0.5388\n",
      "Epoch 144/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9570 - acc: 0.5357\n",
      "Epoch 145/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9622 - acc: 0.5337\n",
      "Epoch 146/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9546 - acc: 0.5354\n",
      "Epoch 147/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9543 - acc: 0.5434\n",
      "Epoch 148/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9564 - acc: 0.5366\n",
      "Epoch 149/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9564 - acc: 0.5397\n",
      "Epoch 150/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9587 - acc: 0.5314\n",
      "Epoch 151/200\n",
      "3502/3502 [==============================] - 7s 2ms/sample - loss: 0.9568 - acc: 0.5420\n",
      "Epoch 152/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9550 - acc: 0.5360\n",
      "Epoch 153/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9579 - acc: 0.5328\n",
      "Epoch 154/200\n",
      "3502/3502 [==============================] - 7s 2ms/sample - loss: 0.9608 - acc: 0.5328\n",
      "Epoch 155/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9568 - acc: 0.5374\n",
      "Epoch 156/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9595 - acc: 0.5383\n",
      "Epoch 157/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9580 - acc: 0.5337\n",
      "Epoch 158/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9580 - acc: 0.5337\n",
      "Epoch 159/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9544 - acc: 0.5440\n",
      "Epoch 160/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9533 - acc: 0.5408\n",
      "Epoch 161/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9549 - acc: 0.5360\n",
      "Epoch 162/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9543 - acc: 0.5383\n",
      "Epoch 163/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9586 - acc: 0.5405\n",
      "Epoch 164/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9553 - acc: 0.5354\n",
      "Epoch 165/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9544 - acc: 0.5423\n",
      "Epoch 166/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9532 - acc: 0.5408\n",
      "Epoch 167/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9580 - acc: 0.5348\n",
      "Epoch 168/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9609 - acc: 0.5357\n",
      "Epoch 169/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9528 - acc: 0.5383\n",
      "Epoch 170/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9529 - acc: 0.5411\n",
      "Epoch 171/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9543 - acc: 0.5411\n",
      "Epoch 172/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9556 - acc: 0.5423\n",
      "Epoch 173/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9634 - acc: 0.5363\n",
      "Epoch 174/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9536 - acc: 0.5388\n",
      "Epoch 175/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9599 - acc: 0.5388\n",
      "Epoch 176/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9532 - acc: 0.5403\n",
      "Epoch 177/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9563 - acc: 0.5366\n",
      "Epoch 178/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9528 - acc: 0.5457\n",
      "Epoch 179/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9560 - acc: 0.5428\n",
      "Epoch 180/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9576 - acc: 0.5340\n",
      "Epoch 181/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9570 - acc: 0.5368\n",
      "Epoch 182/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9536 - acc: 0.5394\n",
      "Epoch 183/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9531 - acc: 0.5403\n",
      "Epoch 184/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9536 - acc: 0.5388\n",
      "Epoch 185/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9546 - acc: 0.5420\n",
      "Epoch 186/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9539 - acc: 0.5411\n",
      "Epoch 187/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9563 - acc: 0.5405\n",
      "Epoch 188/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9550 - acc: 0.5363\n",
      "Epoch 189/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9583 - acc: 0.5348\n",
      "Epoch 190/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9529 - acc: 0.5377\n",
      "Epoch 191/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9537 - acc: 0.5371\n",
      "Epoch 192/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9552 - acc: 0.5408\n",
      "Epoch 193/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9576 - acc: 0.5374\n",
      "Epoch 194/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9547 - acc: 0.5385\n",
      "Epoch 195/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9530 - acc: 0.5385\n",
      "Epoch 196/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9565 - acc: 0.5363\n",
      "Epoch 197/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9551 - acc: 0.5377\n",
      "Epoch 198/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9537 - acc: 0.5408\n",
      "Epoch 199/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9514 - acc: 0.5405\n",
      "Epoch 200/200\n",
      "3502/3502 [==============================] - 6s 2ms/sample - loss: 0.9545 - acc: 0.5380\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_no_stats = tf.keras.Sequential()\n",
    "model_no_stats.add(tf.keras.layers.LSTM(256, input_shape=(1, 10), activation='relu', return_sequences=True))\n",
    "model_no_stats.add(tf.keras.layers.LSTM(128, activation='relu'))\n",
    "model_no_stats.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "model_no_stats.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_no_stats.fit(x_train, y_train, epochs=200, batch_size=20, verbose=1)\n",
    "model_no_stats.save('rnn_no_stats_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model_no_stats.predict_classes(x_test, batch_size=20)\n",
    "# print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5406125\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model_no_stats.evaluate(x_test, y_test, verbose=0)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> France 0.50311524\n",
    "<br> Spain 0.52002466\n",
    "<br> England 0.54775107\n",
    "<br> Germany 0.49465021\n",
    "<br> Italy 0.5406125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
